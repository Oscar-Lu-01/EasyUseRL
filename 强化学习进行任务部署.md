# 使用强化学习进行工作流放置

## 1 Environment

### 1.1 State Space

**状态空间：**状态空间为模型描述了边缘节点集群的样子，例如共有几个节点、每个节点的资源多少、收费如何。本教程中假设的状态空间主要有以下两个层面的描述；

**节点相关状态：**包括集群中每个节点的实际 CPU、内存、网络和磁盘 I/O 带宽利用率；每个节点的 CPU 和内存容量；每个节点的单价；每个节点当前已部署函数实例的最小 CPU 和内存请求总量；每个节点的活跃状态；当前等待调度的函数实例的最小 CPU 和内存请求；

**函数相关状态：**包括当前函数实例的网络和磁盘 I/O 带宽消耗；当前函数实例的请求数；当前函数实例的相对响应时间（RFRT）；集群中每个函数的请求数等;

```python
class ServerlessSchedulingEnv(gym.Env):
    def __init__(self, num_nodes=10, num_functions=5):
        super().__init__()
        self.num_nodes = num_nodes  # 假设集群有10个节点
        self.num_functions = num_functions  # 假设有5种函数类型
        self.task_generator = None  # 任务队列生成器
        self.task_generator_backup = None  # 备份生成器（用于重置）

        # 定义状态空间
        self.observation_space = spaces.Dict({
            # -------------------- 节点相关状态 --------------------
            # 节点实时利用率：[CPU%, 内存%, 网络带宽%, 磁盘I/O%] (shape: num_nodes x 4)
            "node_utilization": spaces.Box(
                low=0.0, high=1.0, 
                shape=(num_nodes, 4), 
                dtype=np.float32
            ),
            
            # 节点容量：[CPU核数, 内存GB] (shape: num_nodes x 2)
            "node_capacity": spaces.Box(
                low=0.0, high=np.inf,
                shape=(num_nodes, 2),
                dtype=np.float32
            ),
            
            # 节点单价（$/小时）(shape: num_nodes,)
            "node_price": spaces.Box(
                low=0.0, high=np.inf,
                shape=(num_nodes,),
                dtype=np.float32
            ),
            
            # 节点已部署资源总量：[已用CPU, 已用内存] (shape: num_nodes x 2)
            "node_deployed": spaces.Box(
                low=0.0, high=np.inf,
                shape=(num_nodes, 2),
                dtype=np.float32
            ),
            
            # 节点活跃状态（0/1表示是否可用）(shape: num_nodes,)
            "node_active": spaces.MultiBinary(num_nodes),
         
            
            # -------------------- 函数实例相关状态 --------------------
            # 当前待调度函数的资源需求：[CPU请求, 内存请求] (shape: 2,)
            "current_function_request": spaces.Box(
                low=0.0, high=np.inf,
                shape=(2,),
                dtype=np.float32
            ),
            
            # 当前函数的I/O需求：[网络带宽MB/s, 磁盘IOPS] (shape: 2,)
            "current_function_io": spaces.Box(
                low=0.0, high=np.inf,
                shape=(2,),
                dtype=np.float32
            ),
            
            # 当前函数的请求数 (shape: 1,)
            "current_function_requests": spaces.Box(
                low=0, high=np.iinfo(np.int32).max,
                shape=(1,),
                dtype=np.int32
            ),
            
            # 当前函数实例的相对响应时间（RART）待商榷
            "current_function_rart": spaces.Box(
                low=0.0, high=np.inf, shape=(1,), dtype=np.float32
            )
            
            # 集群中各函数的请求数 (shape: num_functions,)
            "cluster_function_requests": spaces.Box(
                low=0, high=np.iinfo(np.int32).max,
                shape=(num_functions,),
                dtype=np.int32
            )
        })
```

**关于space的种类, Gymnasium提供了一些类型的space，最常用的就是以下几种：**

- **[`Box`](https://gymnasium.org.cn/api/spaces/fundamental/#gymnasium.spaces.Box)（连续或向量观测空间）：**描述具有任意 n 维形状的上限和下限的有界空间，用于表示连续值或多维向量，例如图像像素、传感器数据、贪吃蛇游戏地图等；
- **[`Discrete`](https://gymnasium.org.cn/api/spaces/fundamental/#gymnasium.spaces.Discrete)（离散动作空间）：**描述一个离散空间，其中 `{0, 1, ..., n-1}` 是我们的观测或动作可以采用的可能值，用于表示有限个离散动作，例如上下左右移动或选择不同操作；
- **[`MultiDiscrete`](https://gymnasium.org.cn/api/spaces/fundamental/#gymnasium.spaces.MultiDiscrete)（多维离散动作）：**由一系列 [`Discrete`](https://gymnasium.org.cn/api/spaces/fundamental/#gymnasium.spaces.Discrete) 动作空间组成，适用于需要多个独立离散动作的场景，例如同时选择速度和方向，一次部署整个工作流可以考虑此space。
- **[`Dict`](https://gymnasium.org.cn/api/spaces/composite/#gymnasium.spaces.Dict)（复合观测空间）：**将多个不同类型的观测组合成字典形式，适用于复杂环境



### 1.2 Action Space

目前先假设单次动作是对单一任务选择节点放置, 动作空间是选择目标节点的索引

**如果是一次放置一整个工作流还需要问一下师兄**

```python
		# 定义动作空间：选择目标节点（索引范围 0 到 num_nodes-1）
        self.action_space = spaces.Discrete(num_nodes)
```



### 1.3 dataset注入

到目前为止，我们拥有了`self.observation_space` ，它定义了状态空间中的shape，**为模型描述了边缘节点集群的样子**（但是仅仅是shape，缺少value，例如模型知道节点单价使用长度为` num_nodes` 的数组表示，但是没有初始化节点单价的值）。

我们需要**读取数据集对状态空间的值进行初始化**，而且需要在**放置完一个函数实例后加载下一个函数实例继续训练**，这一切都离不开dataset注入，下面我们将对environment进行dataset的注入，使其能加载我们的数据集进行训练：

```python
    def set_dataset(self, task_generator):
        """注入任务队列生成器（支持迭代器或生成器）"""
        self.task_generator = iter(task_generator)  # 转换为迭代器
        self.task_generator_backup = self.task_generator  # 备份用于重置

    def _get_next_task(self):
        """获取下一个任务参数（优先从数据集，否则随机生成）"""
        if self.task_generator is not None:
            try:
                return next(self.task_generator)
            except StopIteration:
                # 数据集耗尽，重置生成器（避免训练中断，待商榷）
                self.task_generator = iter(self.task_generator_backup)
                return next(self.task_generator)
```



### 1.4 State Space状态初始化

`self.observation_space` 中仅定义了状态的shape，**在 `reset()` 方法中初始化状态数据**

这里因为还没有数据集, 所以数据均为随机生成的模拟数据，实际初始化需要读取数据

```python
    def reset(self, seed=None):
        # 从数据集或随机生成器获取初始任务参数
        task_params = self._get_next_task()
        observation["current_function_request"] = np.array(
            [task_params["cpu_request"], task_params["mem_request"]], dtype=np.float32
        )
        observation["current_function_io"] = np.array(
            [task_params["io_network"], task_params["io_disk"]], dtype=np.float32
        )
        observation["current_function_requests"] = np.array(
            [task_params["requests"]], dtype=np.int32
        )

        # 模拟初始响应时间（待商榷）
        observation["current_function_rart"] = np.array([0.5], dtype=np.float32)

        self.state = observation.copy()
        return observation, {}
```



### 1.5 动作验证与状态更新

**在 `step()` 方法中处理action，包括三方面：**

- **动作有效性:**  通过` _is_action_valid()`函数判断节点是否满足放置条件
- **奖励函数:**  通过` _calculate_reward()`函数计算奖励值
- **状态更新:** 通过` _get_next_state`函数生成新的状态

整体框架如下，具体三个函数的实现这里也给出了示例（部分逻辑做了简化，但是流程完整）

```python
    def step(self, action):
        # 检查动作是否合法（例如节点是否活跃）
        is_valid = self._is_action_valid(action) 
        # 当前回合是否结束
        done = True if(!is_valid) else False
        # 计算奖励值
        reward = self._calculate_reward(action)
        # 生成新状态
        next_observation = self._get_next_state(action)
        
        # step返回值为以下元组：
        # ------------------------------
        # 1. next_observation ：下一个观测值
        # 2. reward			  ：奖励函数奖励值
        # 3. Termination Flag ：终止标志，指示当前回合是否结束
        # 4. Truncation Flag  ：截断标志，指示当前回合是否因达到某个条件而被截断（如达到最大步数）
        # 5. Info			  ：包含额外的调试信息或辅助数据，不用于训练
        # ------------------------------
        return next_observation, reward, done, False, {}
```



`_is_action_valid()`函数具体实现如下: 

```python
    def _is_action_valid(self, action):
        # 验证节点是否活跃且资源充足
        node_active = self.state["node_active"][action]
        cpu_avail = self.state["node_capacity"][action, 0] - self.state["node_deployed"][action, 0]
        mem_avail = self.state["node_capacity"][action, 1] - self.state["node_deployed"][action, 1]
        cpu_needed = self.state["current_function_request"][0]
        mem_needed = self.state["current_function_request"][1]
        return node_active and (cpu_avail >= cpu_needed) and (mem_avail >= mem_needed)
```



奖励函数如下,  其中，`R1` 是当前函数相对响应时间（RFRT），`R2` 是集群虚拟机使用成本的变化量，`β` 是学习参数, 具体实现参照` _calculate_reward()`:
$$
Reward = -\left( \beta \cdot \frac{R_1 - R_{1,\min}}{R_{1,\max} - R_{1,\min}} + (1-\beta) \cdot \frac{R_2 - R_{2,\min}}{R_{2,\max} - R_{2,\min}} \right)
$$

```python
   def _calculate_reward(self, action):
        """计算调度动作的奖励值"""
        # ------------------------------
        # 1. 提取关键参数和状态信息
        # ------------------------------
        # 从状态中获取当前函数实例的响应时间（RART）和节点单价
        current_rart = self.state["current_function_rart"][0]  # 假设状态中包含 RART
        selected_node_price = self.state["node_price"][action]  # 选中节点的单价

        # 预设参数（可从环境初始化传入）
        beta = 0.7  # 性能权重（超参数，需调优）
        R1_min, R1_max = 0.1, 5.0  # 响应时间合理范围（单位：秒，根据业务设定）

        # ------------------------------
        # 2. 归一化处理（Min-Max Scaling）
        # ------------------------------
        # 归一化响应时间（RART）
        if (R1_max - R1_min) <= 1e-6:  # 避免除以零
            norm_rart = 0.0
        else:
            norm_rart = (current_rart - R1_min) / (R1_max - R1_min)
            norm_rart = np.clip(norm_rart, 0.0, 1.0)  # 限制在[0,1]

        # 归一化节点成本（动态基于集群节点单价）
        all_prices = self.state["node_price"]
        R2_min, R2_max = np.min(all_prices), np.max(all_prices)
        if (R2_max - R2_min) <= 1e-6:
            norm_cost = 0.0
        else:
            norm_cost = (selected_node_price - R2_min) / (R2_max - R2_min)
            norm_cost = np.clip(norm_cost, 0.0, 1.0)

        # ------------------------------
        # 3. 计算加权奖励
        # ------------------------------
        reward = -(beta * norm_rart + (1 - beta) * norm_cost)

        # ------------------------------
        # 4. 可选：惩罚非法动作（若未在 step() 中处理）
        # ------------------------------
        #if not self._is_action_valid(action):
        #    reward -= 10  # 叠加非法动作惩罚,这里仅作为示例,实际惩罚待商榷

        return float(reward)
```



`_get_next_state`函数具体实现如下:

```python
	def _get_next_state(self, action):
        """根据动作生成下一个状态"""
        next_state = self.state.copy()  # 深拷贝当前状态（假设self.state是字典）

        # ------------------------------
        # 1. 更新节点资源部署和利用率
        # ------------------------------
        # 提取当前函数实例的资源需求（CPU和内存）
        cpu_request = self.state["current_function_request"][0]
        mem_request = self.state["current_function_request"][1]

        # 更新选中节点的已部署资源
        next_state["node_deployed"][action, 0] += cpu_request  # 已部署CPU增加
        next_state["node_deployed"][action, 1] += mem_request  # 已部署内存增加

        # 计算新的节点利用率（CPU、内存、网络、磁盘I/O）
        # 假设网络和磁盘I/O容量为固定值（需根据实际业务定义）
        NET_CAPACITY = 1000  # 节点网络带宽总容量（单位：MB/s）
        DISK_CAPACITY = 5000 # 节点磁盘IOPS总容量

        # CPU和内存利用率 = 已部署量 / 节点容量
        next_state["node_utilization"][action, 0] = (
            next_state["node_deployed"][action, 0] / 
            self.state["node_capacity"][action, 0]
        )
        next_state["node_utilization"][action, 1] = (
            next_state["node_deployed"][action, 1] / 
            self.state["node_capacity"][action, 1]
        )

        # 网络和磁盘I/O利用率 = (原使用量 + 函数需求) / 总容量
        next_state["node_utilization"][action, 2] = (
            (self.state["node_utilization"][action, 2] * NET_CAPACITY + 
             self.state["current_function_io"][0]) / NET_CAPACITY
        )
        next_state["node_utilization"][action, 3] = (
            (self.state["node_utilization"][action, 3] * DISK_CAPACITY + 
             self.state["current_function_io"][1]) / DISK_CAPACITY
        )

        # ------------------------------
        # 2. 获得新的待调度函数实例
        # ------------------------------
        # 加载新的函数资源请求，这里没有数据集故随机生成新函数的资源需求（示例逻辑，需替换为实际业务逻辑）
        # 获取新的task实例的数据
        task_params = self._get_next_task()
        # 获取新函数实例的CPU需求、内存需求
        next_state["current_function_request"] = np.array(
            [task_params["cpu_request"], task_params["mem_request"]], dtype=np.float32
        )
        # 获取新函数实例的I/O需求
        next_state["current_function_io"] = np.array(
            [task_params["io_network"], task_params["io_disk"]], dtype=np.float32
        )
        # 获取新函数实例的I/O请求数
        next_state["current_function_requests"] = np.array(
            [task_params["requests"]], dtype=np.int32
        )

        # ------------------------------
        # 3. 更新集群函数请求数
        # ------------------------------
        # 假设调度完成后，当前函数的请求数减少（示例简化逻辑，实际需要替换）
        function_id = np.random.randint(0, self.num_functions)  # 随机选择函数类型
        next_state["cluster_function_requests"][function_id] = max(
            0, 
            next_state["cluster_function_requests"][function_id] - 1
        )

        # ------------------------------
        # 4. 更新响应时间（RART）
        # ------------------------------
        # 模拟响应时间：与节点利用率正相关（示例逻辑，实际需要替换）
        node_load = np.mean(next_state["node_utilization"][action, :2])  # 取CPU和内存负载均值
        next_state["current_function_rart"] = np.array([
            0.1 + 4.9 * node_load  # 响应时间在0.1s（负载0%）到5s（负载100%）之间
        ], dtype=np.float32)

        # ------------------------------
        # 5. 节点活跃状态维护
        # ------------------------------
        # 若节点资源超限则标记为不活跃（示例逻辑）
        cpu_usage = next_state["node_deployed"][action, 0]
        cpu_capacity = next_state["node_capacity"][action, 0]
        if cpu_usage > cpu_capacity:
            next_state["node_active"][action] = 0

        return next_state
```



### 1.6 完整Environment模板

```python
class ServerlessSchedulingEnv(gym.Env):
    def __init__(self, num_nodes=10, num_functions=5):
        super().__init__()
        # 定义状态和动作空间（参考上文）    
                
    def set_dataset(self, task_generator):    
        # 注入任务队列生成器
    def _get_next_task(self):
        
    	#获取下一个任务参数
    def reset(self, seed=None):
        # 初始化状态（参考上文）
    def step(self, action):
        # 处理动作并返回新状态（参考上文）
    def render(self):
        # 可选：实现可视化逻辑
    
    #验证动作有效性
    def _is_action_valid(self, action):
    #计算奖励
    def _calculate_reward(self, action):
    #更新状态
    def _get_next_state(self, action):
```



## 2 完整训练流程

### 2.1 定义Environment

```python
import gymnasium as gym
from stable_baselines3 import PPO
from stable_baselines3.common.env_checker import check_env
from stable_baselines3.common.vec_env import DummyVecEnv
from stable_baselines3.common.evaluation import evaluate_policy
from typing import Callable

# 导入自定义环境类
from serverless_env import ServerlessSchedulingEnv  # 假设环境代码保存在 serverless_env.py

# 定义模拟数据集加载函数（占位符）
def load_dataset() -> Callable:
    """模拟数据集加载逻辑（例如任务队列生成器）"""
    # 实际需替换为真实数据接口（如读取CSV、API等）
    def _task_generator():
        while True:
            # 返回模拟任务参数（CPU、内存、I/O需求等）
            yield {
                "cpu_request": np.random.uniform(0.1, 4.0),
                "mem_request": np.random.uniform(0.1, 8.0),
                "io_network": np.random.uniform(10, 100),
                "io_disk": np.random.uniform(100, 1000),
                "requests": np.random.randint(1, 100)
            }
    return _task_generator()

# 初始化环境
def make_env():
    dataset = load_dataset()  # 获取任务生成器
    env = ServerlessSchedulingEnv(num_nodes=10, num_functions=5)
    env.set_dataset(dataset)  # 假设环境类有 set_dataset 方法注入数据
    return env

env = DummyVecEnv([make_env])  # 向量化环境（支持并行训练）
```

### 2.2 检查Environment的兼容性

```python
# 检查环境是否符合 Gymnasium 接口
check_env(env.envs[0], warn=True)
```

### 2.3 模型定义

本教程使用 PPO 算法（适合连续/高维状态空间），实际使用可替换为 DQN（适用于离散动作空间）或 SAC（连续动作优化），也支持自定义网络；

```python
# 定义策略网络架构（可选自定义）
policy_kwargs = {
    "net_arch": [dict(pi=[128, 128], vf=[128, 128])]  # 策略网络和价值网络结构
}

# 初始化 PPO 模型
model = PPO(
    "MlpPolicy", 
    env,
    learning_rate=3e-4,
    n_steps=2048,          # 每次迭代的步数
    batch_size=64,         # 批大小
    gamma=0.99,            # 折扣因子
    verbose=1,             # 日志输出等级
    tensorboard_log="./logs/",
    policy_kwargs=policy_kwargs
)
```

### 2.4 模型训练

```python
# 训练步数
total_timesteps = 100000
model.learn(
    total_timesteps=total_timesteps,
    tb_log_name="ppo_serverless",
    progress_bar=True
)

# 保存模型
model.save("ppo_serverless_scheduling")
```

### 2.5 模型评估

```python
# 创建评估环境
eval_env = DummyVecEnv([make_env])

# 评估策略（10 次轨迹的平均奖励）
mean_reward, std_reward = evaluate_policy(
    model,
    eval_env,
    n_eval_episodes=10,
    deterministic=True
)
print(f"Mean reward: {mean_reward:.2f} ± {std_reward:.2f}")
```

### 2.6 模型推理

```python
# 加载模型
model = PPO.load("ppo_serverless_scheduling", env=env)

# 单次推理示例
obs, _ = env.reset()
for _ in range(1000):  # 模拟 1000 步调度
    action, _ = model.predict(obs, deterministic=True)
    obs, reward, done, info = env.step(action)
    if done:
        obs, _ = env.reset()
```



## 你需要做的：

- **数据集及其加载逻辑：**数据集可以找一个工作流的数据集先进行测试，state space可以进行适当删减来简化实验，主要目的是测试数据初始化、space更新过程，把流程打通
- **设计奖励函数：**需要好好设计一下用什么约束，时间敏感型可以考虑时延+成本，准实时任务可以考虑SLO+成本。很重要但是不涉及流程问题，换一下就行
- **模型选择：**换模型也很快的，有框架无缝换
